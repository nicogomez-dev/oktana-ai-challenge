{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-Bm3jtfQITlVhmSSDIwmayVSbQ0qb5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks that facilitate building applications with large language models (LLMs), but they serve slightly different purposes and have distinct features.\\n\\n**1. Purpose and Focus:**\\n\\n- **LangChain:**\\n  - Focuses on building ** conversational applications** and **agent-based systems**.\\n  - Provides a toolkit for orchestrating prompts, managing memory, chaining multiple language model calls, and integrating with external tools and APIs.\\n  - Emphasizes **composability** and **workflow management** for complex LLM applications.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Specializes in **building indices** over external data sources like documents, PDFs, or knowledge bases.\\n  - Enables **efficient retrieval** and **question answering** over large external datasets by indexing them and conducting semantic search.\\n  - Focuses on **data ingestion**, **indexing**, and **querying** to augment LLM capabilities with external knowledge.\\n\\n**2. Core Capabilities:**\\n\\n| Feature | LangChain | LlamaIndex |\\n|---|---|---|\\n| Workflow management | Yes | No (focused on data indexing) |\\n| Prompt engineering | Yes | Yes (but more geared towards indexing and retrieval) |\\n| External API/tool integration | Yes | No (primarily data indexing) |\\n| Memory management/storage | Yes | No (unless using external tools) |\\n| Data ingestion & indexing | Limited | Yes |\\n| Retrieval-augmented generation (RAG) | Can be implemented | Yes, core feature |\\n\\n**3. Typical Use Cases:**\\n\\n- **LangChain:**\\n  - Building chatbots, virtual assistants, or agents that can interact with APIs or perform multi-step reasoning.\\n  - Managing complex conversational workflows.\\n  - Orchestrating calls to multiple LLMs or tools.\\n\\n- **LlamaIndex:**\\n  - Building question-answering systems over large document collections.\\n  - Creating knowledge bases for retrieval-augmented tasks.\\n  - Indexing and querying documents, PDFs, or other data repositories.\\n\\n**4. Integration and Ecosystem:**\\n\\n- **LangChain** is language-model agnostic and supports integrations with many LLM providers (OpenAI, Azure, Cohere, etc.) and tools.\\n- **LlamaIndex** primarily works with document stores and aims to augment LLMs with external data.\\n\\n---\\n\\n**In summary:**\\n\\n- **Use LangChain** if you're aiming to **build complex conversational workflows, agents, or multi-step LLM applications** that involve orchestration, memory, and tools.\\n- **Use LlamaIndex** if you're **building retrieval systems** over large datasets or documents, enabling LLMs to **answer questions using external knowledge** efficiently.\\n\\nMany projects combine both frameworks—using LlamaIndex for data indexing and retrieval, and LangChain for orchestrating interactions and workflows—leveraging the strengths of each.\\n\\n---\\n\\nIf you'd like specific guidance on choosing or integrating these tools, feel free to ask!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750793817, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_f12167b370', usage=CompletionUsage(completion_tokens=614, prompt_tokens=19, total_tokens=633, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both frameworks designed to facilitate building applications that leverage large language models (LLMs), but they serve somewhat different purposes and offer different functionalities. Here's a breakdown of their main differences:\n",
              "\n",
              "### 1. **Primary Focus and Use Cases**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Focuses on **building complex, multi-step, conversational, or agent-based applications** with LLMs.\n",
              "  - Provides abstractions for chaining together prompts, managing memory, handling conversation states, and integrating with various APIs and tools.\n",
              "  - Ideal for developing chatbots, virtual assistants, or applications requiring dynamic interactions and reasoning over multiple steps.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - Primarily designed for **indexing, searching, and retrieving information from large external data sources** like documents, PDFs, or databases to answer queries.\n",
              "  - Acts as a bridge to utilize LLMs for **semantic search, question answering, and knowledge base creation** over custom data.\n",
              "  - Suitable for building knowledge bases, document QA systems, or retrieval-augmented generation (RAG) workflows.\n",
              "\n",
              "### 2. **Core Functionality**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Provides **building blocks** such as prompt templates, chains, agents, memory, and tool integrations.\n",
              "  - Enables **orchestration** of many steps including calls to LLMs, external APIs, or tools.\n",
              "  - Supports complex workflows like multi-turn conversations, agent prompts, and dynamic decision-making.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Offers **data ingestion and indexing capabilities** to convert raw data into queryable formats.\n",
              "  - Contains **connectors and wrappers** for various data sources.\n",
              "  - Focuses on **semantic search** and **retrieval-augmented generation**, where LLMs generate responses based on relevant parts of the indexed data.\n",
              "\n",
              "### 3. **Level of Abstraction and Flexibility**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Higher-level framework for designing **interactive and multi-step applications**.\n",
              "  - Offers detailed control over prompt engineering, dialogue flow, and integration with external tools.\n",
              "  - Emphasizes **custom workflows** and complex orchestration.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - More specialized towards **efficient data management** and retrieval.\n",
              "  - Simplifies the process of creating searchable indices over data, then querying them with LLMs.\n",
              "  - Focuses on **ease of use** for integrating external data into language model workflows.\n",
              "\n",
              "### 4. **Community, Ecosystem, and Adoption**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Has a large and active community focused on conversational AI, chatbots, and multi-step reasoning.\n",
              "  - Extensive documentation and a wide array of integrations.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Gaining popularity in contexts where external data needs to be incorporated into LLM workflows.\n",
              "  - Often used in enterprise, research, and knowledge management systems.\n",
              "\n",
              "---\n",
              "\n",
              "### **In summary:**\n",
              "\n",
              "| Aspect                     | **LangChain**                                      | **LlamaIndex**                                |\n",
              "|----------------------------|-----------------------------------------------------|------------------------------------------------|\n",
              "| Main purpose               | Building complex, multi-step LLM applications      | Indexing/searching external data for LLMs   |\n",
              "| Focus                      | Orchestrating prompts, chats, tools, and memory    | Data ingestion, retrieval, QA over data     |\n",
              "| Ideal for                  | Chatbots, agents, conversational AI                | Knowledge bases, document search, RAG      |\n",
              "| Abstraction level          | High; flexible and customizable                     | Focused on data indexing and retrieval     |\n",
              "\n",
              "---\n",
              "\n",
              "**In essence:**  \n",
              "- Use **LangChain** if you're building an application that requires complex workflows, multi-turn conversations, or integration of various tools and APIs.  \n",
              "- Use **LlamaIndex** if you're aiming to retrieve and reason over large external datasets to support querying or question answering.\n",
              "\n",
              "---\n",
              "\n",
              "If you need a recommendation for a specific project or use case, feel free to share more details!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I can't believe I have to choose between crushed ice and cubed ice when I'm this angry and starving! Neither sounds appealing right now—just get me some real food already!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is great for a refreshing, cool treat, especially in drinks like snow cones or cocktails. Cubed ice is perfect for keeping beverages cold without diluting them too quickly. Both have their charms—what do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Bm3sUbUR44Fv1db5XliDYrmqoeK2D', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice is great for a refreshing, cool treat, especially in drinks like snow cones or cocktails. Cubed ice is perfect for keeping beverages cold without diluting them too quickly. Both have their charms—what do you prefer?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750794350, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=48, prompt_tokens=30, total_tokens=78, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Here's a sentence using both words:\n",
              "\n",
              "\"During the strange village festival, I encountered a stimple of falbean, which added to the mysterious atmosphere of the event.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench effortlessly tightened the falbean, ensuring the assembly spun smoothly without any wobble."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze Billy's options carefully.\n",
              "\n",
              "**Given data:**\n",
              "\n",
              "- Current time: 1 PM local time\n",
              "- Deadline: 7 PM EDT\n",
              "- Travel options:\n",
              "  1. **Fly (3 hrs)** + bus (2 hrs) = total 5 hours\n",
              "  2. **Teleporter (0 hrs)** + bus (1 hr) = total 1 hour\n",
              "\n",
              "---\n",
              "\n",
              "**Important considerations:**\n",
              "\n",
              "- The local time is 1 PM now.\n",
              "- The deadline is 7 PM EDT, so the latest he can arrive **EDT** time is 7 PM.\n",
              "- **Are the local time and EDT synchronized?**  \n",
              "  The key question is: *Is the current local time given in local time zone or EDT?*\n",
              "\n",
              "**Assumption:**  \n",
              "Since the problem states \"It's currently 1PM local time,\" and the deadline is \"before 7PM EDT,\" I will assume:\n",
              "\n",
              "- The local time is in San Francisco's time zone (Pacific Time, PT).\n",
              "- EDT is Eastern Daylight Time, which is ahead of PT by 3 hours.\n",
              "\n",
              "**Time zone difference:**  \n",
              "- Pacific Time (PT): UTC-7 or UTC-8 depending on daylight saving,  \n",
              "- EDT: UTC-4.\n",
              "\n",
              "**During daylight saving time**, Pacific Time is UTC-7, EDT is UTC-4, so EDT is ahead of Pacific Time by 3 hours.\n",
              "\n",
              "**Convert current local time to EDT:**\n",
              "\n",
              "- Current local time: 1 PM PT.\n",
              "- In EDT, this is 1 PM + 3 hours = 4 PM EDT.\n",
              "\n",
              "---\n",
              "\n",
              "**Calculate total travel times in EDT:**\n",
              "\n",
              "### Option 1: Fly + bus\n",
              "\n",
              "- Total travel time: 3 + 2 = 5 hours\n",
              "- Starting at 1 PM PT (which is 4 PM EDT), arriving **after 5 hours**:\n",
              "\n",
              "  - Arrival EDT time = 4 PM + 5 hours = 9 PM EDT\n",
              "\n",
              "### Option 2: Teleporter + bus\n",
              "\n",
              "- Total travel time: 0 + 1 = 1 hour\n",
              "- Starting at 1 PM PT (4 PM EDT):\n",
              "\n",
              "  - Arrival EDT time = 4 PM + 1 hour = 5 PM EDT\n",
              "\n",
              "---\n",
              "\n",
              "**Does it matter which option Billy selects?**\n",
              "\n",
              "- **Fly + bus:** arrives at 9 PM EDT (after deadline 7 PM EDT).\n",
              "- **Teleporter + bus:** arrives at 5 PM EDT (before deadline).\n",
              "\n",
              "**Conclusion:**\n",
              "\n",
              "Yes, it **does matter** which option Billy selects because:\n",
              "\n",
              "- The **teleporter + bus** option gets him home **before** 7 PM EDT.\n",
              "- The **fly + bus** option **cannot** arrive before the deadline.\n",
              "\n",
              "**Therefore, Billy should choose the teleport + bus option** to ensure he gets home before 7 PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the options step by step.\n",
              "\n",
              "**First, understand the current time and target:**\n",
              "\n",
              "- Current local time: 1PM\n",
              "- Billy wants to arrive **before 7PM EDT**.\n",
              "\n",
              "**Important:** We need to verify if the local time is EDT or if there's a time difference.\n",
              "\n",
              "Given that Billy is in San Francisco (Pacific Time Zone), and the deadline is in EDT (Eastern Time Zone):\n",
              "\n",
              "- San Francisco (Pacific Time): UTC-8 (Standard Time)\n",
              "- Eastern Time: UTC-5 (Standard Time)\n",
              "\n",
              "Time difference: 3 hours (Eastern is ahead of Pacific)\n",
              "\n",
              "**So, if it's 1PM local (Pacific), then in EDT it is:**\n",
              "\n",
              "- 1PM Pacific + 3 hours = 4PM EDT.\n",
              "\n",
              "**Therefore, Billy needs to arrive **before 7PM EDT**, and the current EDT time equivalent is 4PM.**\n",
              "\n",
              "---\n",
              "\n",
              "### Step 1: Determine total travel times for each option\n",
              "\n",
              "**Option A: Fly + Bus**\n",
              "\n",
              "- Flying takes 3 hours.\n",
              "- Bus takes 2 hours.\n",
              "- Total: 3 + 2 = **5 hours**\n",
              "\n",
              "**Option B: Teleporter + Bus**\n",
              "\n",
              "- Teleporter: 0 hours.\n",
              "- Bus: 1 hour.\n",
              "- Total: 0 + 1 = **1 hour**\n",
              "\n",
              "---\n",
              "\n",
              "### Step 2: Calculate departure times needed to arrive before 7PM EDT\n",
              "\n",
              "Since the current time in EDT is 4PM, and Billy must arrive **before 7PM**, that means he has:\n",
              "\n",
              "- 7PM - current EDT time (4PM) = **3 hours** remaining from now until 7PM.\n",
              "\n",
              "Let's see if both options can get Billy home **before 7PM**.\n",
              "\n",
              "---\n",
              "\n",
              "### **Option A: Fly + Bus**\n",
              "\n",
              "- Travel time: 5 hours.\n",
              "- Earliest departure time in EDT: current time (4PM) plus travel time.\n",
              "\n",
              "To arrive before 7PM, he must depart **no later than**:\n",
              "\n",
              "- Arrival deadline (before 7PM) minus 5 hours.\n",
              "\n",
              "But since it's 4PM now:\n",
              "\n",
              "- If Billy departs **immediately at 4PM EDT**, he will arrive:\n",
              "\n",
              "  - 4PM + 5 hours = **9PM EDT**.\n",
              "\n",
              "- This is **after** the desired 7PM deadline.\n",
              "\n",
              "Therefore, Billy **must depart earlier than 4PM EDT**, which is impossible because it's 4PM now.\n",
              "\n",
              "*Conclusion:* with current time being 4PM EDT, **Billy cannot achieve arrival before 7PM with the fly + bus option unless he departs before now**. Since it is already 4PM, he cannot do better unless he departs earlier.\n",
              "\n",
              "---\n",
              "\n",
              "### **Option B: Teleporter + Bus**\n",
              "\n",
              "- Travel time: 1 hour.\n",
              "\n",
              "- From current EDT time (4PM), arrival will be:\n",
              "\n",
              "  - 4PM + 1 hour = **5PM EDT**.\n",
              "\n",
              "- Since 5PM is before 7PM, Billy can arrive on time **by taking this option and departing immediately**.\n",
              "\n",
              "---\n",
              "\n",
              "### **Summary:**\n",
              "\n",
              "- **The teleporter option can get Billy home before 7PM if he departs immediately**, because arriving at 5PM EDT is well before the 7PM deadline.\n",
              "\n",
              "- **The fly + bus option would require departure before 4PM EDT**, which is impossible because it's already 4PM.\n",
              "\n",
              "**Therefore:**\n",
              "\n",
              "- If Billy starts right now (1PM local / 4PM EDT), he **must** take the teleporter + bus to arrive on time.\n",
              "- The fly + bus option **cannot** guarantee timely arrival unless he departs earlier than now, which isn't possible.\n",
              "\n",
              "---\n",
              "\n",
              "### **Answer:**\n",
              "\n",
              "**Since it's currently 1PM local time (4PM EDT), Billy cannot arrive before 7PM EDT by flying + bus. He can arrive on time by taking the teleporter + bus.**\n",
              "\n",
              "**In general:**\n",
              "\n",
              "- If both options are available immediately, the teleporter + bus guarantees arrival before 7PM due to shorter total travel time (1 hour vs. 5 hours).\n",
              "- **Yes, it does matter** which option Billy selects because the more time-consuming option (fly + bus) cannot meet the deadline if starting now, but the teleporter + bus can.\n",
              "\n",
              "---\n",
              "\n",
              "**Final note:** If the current time were earlier, say before 2PM local (before 5PM EDT), then the fly + bus might be feasible. But given the current time specified, the teleporter + bus is the only viable choice to meet the deadline."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
